---
templateKey: blog-post
title: Introduction to statistics
date: 2019-01-25T18:56:15.985Z
description: >-
  This is a story, inspired by the Introduction to Statistics Course in the
  Biomedical Sciences Bachelor, Maastricht University.
tags:
  - Test
---
Why do we need statistics?

Statistics gives us the mathematical tools to determine how the collected data should be interpreted. It allows us to weight the findings against natural randomness. It provides Science with an objective view on the reliability of every study.

•	Significance, Variance, P-value, T-Distribution

In this entry i will compress the topics covered in this 4 week long course into a story:



![](/img/lg-1.png)

The story of a statistician

<H4> Introducing variance </H4>

The scientist approaches the statistician with the following question:

I feel like my data varies a lot, can you give me a number?

The task is: Find the level of determination.

There are a number of ways to determine the level of variability in data sets. A very intuitive way would be to just take the absolute distance of every data point tot he mean and then divide it by the number of data points. The problem that arises is that mathematical operations like taking the derivative or integrating is very hard with absolute values.

The second way solves this, instead of taking the absolute distance, the distance to the mean gets squared. This way still ensures that negative and positive differences dont cancel each other out and it makes the life of mathematicians easier.  Variance

The problem with this is, that the interpretation of the obtained value is not that intuitive.

For example: If we talk about variance in height: if the average absolute distance (way 1) would be 10 cm. The variance will be 100cm(squared). So the variance cant be interpreted as easy.

The third way solves this: by taking the root of the Variance, we obtain the standard deviation. This means now the standard deviation is equal to 10 cm and this can be normally interpreted as it has the same unit as the investigated variable (height- cm).



## Covariance

Now the statistician wants to go a step further: He want to investigate the level of determination between 2 variables. We just talked about the average distance of one variable to its mean, now we want to establish if 2 variable vary in the same way, that could give us inside about a possible relationship between those two. The statistician comes up with the covariance: It measures to what extend the variables vary in the same way: We make the sum of the distance of xi with its mean and multiply it with the distance of yi from its mean. Then we divide it by the number of the data points.

Why does this do the trick?

If we think about it: We want the covariance to be maximal only if the variables do exactly the same or exactly the opposite. This way the variables behave so, that we could know the value of one variable based on the other. The formula does this by multiplying the distance to the mean of one variable data point with the corresponding data point of the other variable. And then taking the sum: By summing up the values, lets think about whats happens:

First the extreme cases: If the variables behave exactly the same. 

The average distance to the mean would be either positive or negative for both. Mutiply to positive numbers or two negative numbers, the result is always positive. 

Now if the variables behave exacly opposite from each other: The product of the average distances would be always negative, because if it would be positive for x, the corresponding value for y would be negative and vice  versa.

Now lets consider that the two variables are completely independent. That way, the covariance will be zero, as on average the variables will behave the same and the opposite in the same proportions.

Now that we understood the extreme cases, it is easy to understand why in the real world cases the covariance gives us a precise value for the relationship. In real world cases the variables behave the same for some data points and opposite for other data points, in this way the covariance will be smaller. The more one of these cases takes precedence over the other the closer the covariance will be to its maximum.

The problem with the covariance (notice a pattern: solution-> problem -> solution ->problem..):

The value of the covariance is dependent on the units of the variables, that means that the covariance for variables in kilometres and kilos will be very different from the covariance of the same variables in metres and grams. This would make it very hard to compare the covariances of different cases, as in each case we would have to first look at the units, then switch them around so the units match the case we want to compare it to.

So we should be able to normalize the covariances and relieve it from its dependency.

Said and done: (Mathematicians are nice peoples):

##  The pearson correlation coefficient:

The pearson correlation coefficient achieves exactly that, it divides the covariance by the standard deviation of both variables and therefore eliminates the units of the covariance.

Another nice thing about this correlation coefficient (it asks: what is the level of correlation??) is that it ranges exactly from -1 to 1. Now you ask why does it do that? Well: The standard deviations are always positive, it is equal to calculating the covariance with absolute values for the difference. So the actual covariance will always be smaller or equal to the product of the standard deviations. It will be equal in the extreme positive or negative cases (r=1 and  r=-1) Insert picture

Now that the statistician know about the reliationship between those 2 variables, he wants to go a step further by crafting a model that allows us to predict the value of one variable based on the the other variable. The prerequisite for such prediction is would be the formulation of the exact relationship between the 2 variables.

Solution Rsquared: The R-squared regression enables us to find the mathematically best fitting model to our data and has therefore the highest chance of generating helpful predictions. 

This is accomplished by maximizing Rsquared. Why that? Well R-squared is conveniently defined as exactly what we need: As the variance that is explained by our model over the total variance. 

To get a better feeling for what is happening here, lets consider the extreme cases.

So if R-squared is maximal, our model explains the entire variance. This would be the case if all of our data points fall exactly on our Regression Line. The relationship between the variables would be exactly linear.

In the worst case there is no relationship between the variables, in this case R-squared is 0 because the Regression Line cannot explain any of the observed variance.

\[sum of squares regression]

<H3> Categorical Variables </H3>

![](/img/lg-2.png)

Let’s consider a study like this:

We want to investigate wether a certain factor causes a certain outcome :D 

The scientists observed 2 groups: 1 group consists of smokers and one group consists of non-smokers. Then after 20 years he established which participants developed lung cancer and which did not. To present the data he provides you a 2x2 table 

```
Cancer	No-Cancer
```

Smoker	10	90

Non-Smoker	5	95

The task is to determine wether there is a relationship between cancer and smoking.

The statistician has an idea how to tackle this problem:

He says, okay let’s pretend for a minute that there is no relationship between the two variables. How many people would fall in each category then?

Well, 15 people got cancer, so we would expect them to fall equally in the categories smoker and non-smoker. So the new table would look like this:

```
Cancer	No-Cancer
```

Smoker	7.5	92.5

Non-Smoker	7.5	92.5

Now, let’s compare the two tables:

<h3>The Chi-Square Test </h3>

The statistician is close to developing the chi-square test, which comes from exactly this idea: Lets pretend there is no relationship, generate the table according to this and then compare it to the actual table.

\[Insert Chi- square formula]

And chi.square table

<h3>Relative risk and Relative Odds</h3>

The scientist is very happy with the findings, but he gets greedy. He wants to know the risk of getting cancer when smoking compared to when not smoking

The statistician is in the flow, he quickly comes up with a way to satisfy the scientists needs:

He calculates the probability of getting cancer when smoking, and then divides it by the probability in the non-smoker group.

Insert Formulas

<h4>The Odds</h4>

The odds are another, less intuitive way, of making sense of categorical data. Risk and Odds differ, the Risk calculates the absolute Frequency of an incident over the whole group. The odds expresses the relative frequency. 

So in this case: ….

## Risk in Case Control Studies

One important limitation of the risk is that it cant be generated from Case Control studies:

Let’s understand this based on an example:

```
Smoker	Non-Smoker
```

Cancer	60	40

No-Cancer	10	90

In this study 100 cancer patients were selected and 100 No-cancer participants were the control group. If we would determine the risk of getting cancer when smoking in this context, It would be 60/70*100= 86 percent.

That is obviously wrong, because we determined the risk by our case-control study design.

Nevertheless, there seems to be information to be extracted from this study. It is obvious that under the cancer patients the fraction of Smokers is higher than in the Non-Cancer group. 

We cannot ask the question, what is the risk of getting cancer? But we can ask the question: What are the odds of a participant being a smoker when he is a cancer patient compared to when he is not having cancer?

This can be calculated by determining the relative frequency of Smokers in the cancer group over the relative frequency Smokers in the No-Cancer group. So (60/40)/(10/90)= 13.5

So in this sample, the odds to be a smoker are 12.5 times higher if you are a cancer patient than if you are not.

<h3>Sample/population/sampling Distributions</h3>

A young ambitious scientist wants to once and for all determine the average blood pressure of people in the Netherlands. He starts measuring, measures and measures some more.

Once he reached a thousand measurements, he has a nervous breakdown. He feels like he will never be able to reach his goal as he calculates that he only managed to measure 0.0005 percent of the dutch people so far.

In his desperateness he approaches our dear statistician, confronting him with his story.

The statistician takes his time, looks at the data the young scientist already collected, calculates a bit and comes back to the scientist with a big smile: “You already reached you goal!”

He explains: You collected a sample of the population and based on statistical models and theorems we can determine how likely it is that our sample distribution (The datapoints plotted on a graph) is a good representation of the population distribution.

In your case the numbers are very promising. The sample mean +- 0.1 mmHg with your size and standard deviation covers the true population mean in 95%  of the cases. And if you are fine with a larger confidence interval, that is if you would be fine with saying the population average blood pressure lays between 130 and 131, you could be 99.9 percent confident to be telling the truth.

That makes the scientist very happy. But now he wants to understand how the statistician can come up with these numbers as there seems to be a special kind of witchcraft at work.

So, let’s dive deeper into Sample/Population/sampling distributions and the central limit theorem.

## The Question is: How to generalize Results from a Sample?

Intuitively it should make sense that if you collect a random sample from a population, that from a certain size upwards you can be pretty sure what the population looks like. Imagine you travel through the Netherlands and see a lot of different people. If you would measure for example the height of every person you see and calculate the average, you would be very confident that you know dutch peoples height very well.

Now mathematically that is captured by the central limit theorem. It states that: Given you have a population with a finite variance and your sample size is large enough, the sampling distribution will approximate a normal distribution with the mean equal to the population mean and standard deviation equal to the sample standard deviation divided by the square root of the sample size. If the population distribution is normally distributed, the sample size must only be at least 2. If the population distribution is not normally distributed, the sampling distribution will only approximate a normal distribution for large enough sample sizes, it will be equal to a normal distribution if the sample size is infinite. Be careful: If the population distribution is finite, then the sampling distribution will be equal to a degenerate normal distribution (only one value- the population mean), but these are theoretical mathematical cases that don’t play a big role in ‘down to earth’ science.

So, the sampling distribution is equal to the distribution of sample means of the same population and the same size. Since it is normally distributed for large enough sample size, we can make statistical inference about our sample.

An exactly normally distributed sampling distribution is known as Z-Distribution. 

We can infer (like in every other normal distribution) that 95% of the values lie within the confidence interval (the mean ± 1.96*standard error). In this case standard deviation is called the standard error, because we are talking about the sampling distribution.

The width of the the confidence interval is determined by the standard error (which is determined by the standard deviation of the sample means and the sample size) and the confidence level, in our case 95% (1-α).

Be careful: If your sample size is very large, even small differences in the investigated variable will become significant (confidence interval gets very narrow), but this does not automatically translate to meaningful findings.



## The t-distribution:

The t-distribution is a drunken z-distribution: There is a nice background story regarding the t-distributions: William Gosset was working at Guiness where his task was the quality control of the beer production. As he was not allowed to drink excessive amounts of beer, his sample size was very little in comparison to the vast amounts of beer that were produced by guiness.

The formula of the standard error contains the population standard deviation, and we normally (point-) estimate it by the sample standard deviation. Gosset figured out, that 1. There was a price to pay for that impudence and 2. What it costs.

He came up with the t-distributions, these where adjusted for the fact that we are estimating the population standard deviation. For very large samples, the t-distributions come very close to the z-distribution/perfect normal distribution. But for smaller samples the confidence intervals of the t-distributions are noticeably larger than the corresponding interval of the z-distribution.

So thank you Mr. Gosset for being a bean counter! 

The normal distribution:

 

![](/img/lg-3.1.png)

*  Week 4:
  ![](/img/week4.png)
